import pandas as pd

2 basic pandas data structures: Series and DataFrames
Series: one dimensional array, somewhere between a list and a dictionary. Like lists (and ndarrays), except they have an associated array of data labels instead of just index locations. Single indexed column of data. Items are stored in an order and there are labels with which they can be retrieved.

a = [1,2,3] # declare a list
x=pd.Series(a, index=['First','Second','Third']) # stores a list of 3 numbers and labels them, so they can be retrieved either by x[index] or x.label.

underneath, pandas stores data in a numpy typed array. Numbers are stored by default as float64, strings as object.
Dictionaries can also be converted to pandas Series, no need to label.

a = pd.Series([1,2,3,4]) # converts a list to a series
b = pd.Series([1,2,3,4], index=['a','b','c','d']) # creates the same series, but now with custom index labels
a.values # prints the values of the Series
b.index # prints the indexes
b[['c','a','d']] # returns correponding values
Numpy operations like filtering with boolean array, scalar multiplication, math functions etc. apply to Series as well.
Dicts can be converted to Series as:
Series(<dict variable>) # where dict var is {'key1': 'val1', 'key2':'val2' etc}
Series with the same indexes can be aligned (values summed)
A Series' index can be altered in place by assignment to object.index eg. a.index = ['w', 'x', 'y', 'z'] 

a.isnull() will show as True any keys that have no value

Vectorization: do an operation on all elements of a data structure without looping.
np.sum(a) # will calculate the sum of all elements in the series a without looping through them
much faster than looping

Two series can be added - the values of the common indexes will be added and those of the unqiue indexes will be set to Null
###################################
DataFrame: Tabular, spreadsheet like data structure containing an ordered collection of columns each of which can be a different value type (numeric, string, bool etc). Each column in a data frame is a series.

In general, you could say that the Pandas data frame consists of three main components: the data, the index, and the columns.

df = pd.DataFrame(data=<data as a dict or matrix>, index=<list of index values>, columns=<list of columns in order>)

A dict of equal length lists can be converted to a DataFrame
data = {'state': ['OH', 'OH', 'OH', 'NV', 'NV'], 
	'year':[2000,2001,2002,2001,2002], 
	'pop': [1.5,1.7,3.6,2.4,2.9]}

df = pd.DataFrame(data)
df = pd.DataFrame(data, columns=[<list of data columns in order you want>])
df.index =[<list of row names you want>] # will name the rows
# all in one shot
df  = pd.DataFrame(data, columns=[<list of col names>], index=[<list of row names>])

Columns can be pulled out either as df['column'] or df.column
df.columns gives the column names and types
Rows can be selected by df.ix[<rownum>]
Rows can be named with the 'index' function:
df.index=['a','b','c','d'] will give these letter as rownames and can be retrived by df.ix['a'] etc.
df.iloc[index] # used to retrieve by index, same as df[index] or df.ix[index]
df.loc['label'] # used to retrieve by label, same as df['label']

A data frame can be formed by combining a set of equal length pd.Series, ndarrays, lists or dicts of lists (in which the keys will automatically be the column names as above
df = pd.DataFrame([series1, series2, series3])
df2 = df.append(<list>) # will create y adding a new list to df, but won't change df

#Access data based on labels (row/column names)
df.loc[rowname, colname] # will pull out the value in that cell, same as R data.frame
df.loc[:,[col1, col2]] # pulls out all rows for the 2 cols mentioned
#Access data based on indexes/positions
df.iloc[1:4, 3:5] # fetches rows 1,2 and 3 of column numbers 3&4
#For access with a mix of labels and positions, use ix
df.ix[1:4, 'fromcolumnname':'tocolumnname']

# To slice a chunk (some number of rows for a certain set of columns)
df.loc[fromrowname:torowname, fromcolumnname:tocolumnname]
# to get all rows for a select set of columns, use a single colon for rows
df.loc[:, 'fromcolumnname':'tocolumnname']

# copy/delete columns
df.drop(rowname) # won't drop the row from the original df
df.drop(rowname, inplace=True) # will drop from original
df.drop(df[index], inplace=True) # better general syntax
df.drop(df.index[:2], inplace=True) # drops first 2 rows
df.drop('columnname', axis=1 inplace=True) # drops a column by name
df.drop('rowname', 0, inplace=True) # 0 means row
#To delete by index instead of names/labels
df.drop(df.index[from:to, 0] # change to 1 for columns

del df[col] # deletes columnn

df.columns - gives list of columns
list(df.columns) # gives the column names as a list
list(df) # short version of above
list(df.index) # gives names of row labels

# rename columns 2 ways
df.columns=[<list of new column names>]
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)

df.shape # gives number or rows and columns in a data frame
df.info() # describes the structure of the data frame in details, like R str()
df.T will transpose the data frame, same as df.transpose()
#To add a column to a data frame
df['newcolumn'] = "value" # value will be propagated through all the rows of the data frame
if the values are different, they need to be a list
To add a new row, transpose the df, add a column and then transpose back
---------------------------
conditional selection
df[df>0] # will retrieve values from those cells where value > 0, rest returned as null
df[df['col'] > 0] # picks only rows where 'col' > 0
df[df['col1' > 0]['col2'] # fetches just col2 of the above conditional fetched df
df[df['col1' > 0][['col2','col3']] # fetches col2 and col3 of above
df[(df['col1 > 0]) & (df[col2] > 1)] # fetches based on multiple conditions. & works, 'and' doesn't
df[(df['col1 > 0]) | (df[col2] > 1)] # multiple conditions 'or' (word doesn't work)
---------------------------
#To index with a specific column
df.set_index('colname') 
# above does not change the dataframe, just spits out a modified df. To actually change the df:
df = df.set_index('column') #or
df.set_index('column', inplace=TRUE)
df.set_index([list of cols]) # index with multiple columns
df = df.reset_index(inplace=True) # will reset index to numerical range and turn previous index into a regular column
df.unstack('column') # splits a dataframe according the values of the column specified
---------------------------
Mutilevel index - multiple levels of indexing
Passing a tuple of indexes to the index definition when creating a df, the order of elements in the tuple determine the level of indexing ie. first element is the outermost index, second is next etc.
To retrieve elements from df indexed this way
df.loc['outer index'].loc['inner index] # chain indexes with loc
df.xs('inner index', level='name of inner index') # allows to cross outer index and get inner index from multiple outer indices.
---------------------------
Missing values
By default pandas fills in missing values with null (np.NaN)
dropna() allows for dropping of missing values ie drop rows or columns that have missing values
fillna() allows for filling missing values with a desired value
df.dropna() will drop rows that have missing values
df.dropna(axis=1)
df.dropna(thresh=2) # drop rows that have atleast 2 nulls
df.fillna(value='fillval') # fills in all NaNs with 'fillval'
df.fillna(value=df['col'].mean()) # fills NaNs with mean of 'col'
---------------------------
Querying a DataFrame:
By creating boolean arrays
df[col] > 0 # will create a boolean array where every entry that satisfies the condition will be true and the rest false. This array can be super-imposed on the dataframe to query with the 'where' clause:

df.where(df[col] > 0) # same as in R
df[df[col]>0] # same as above

Counting number of entries that match a certain column value:
df[df['col']=='value']['col'].count() # the second value counts for a single column, otherwise it gives the same count for all columns 

df[col].unique() # unique values from a col
df = df[df[col] == <some value>] # recreates the data frame with only those entries that satisfy the condition
df = df[<list of cols to keep>] # edits dataframe to drop all other cols
---------------------------
Groupby
Similar to SQL group_by, allows you to group by a certain column - ie aggregate rows based on a certain column by running an aggregate function on other columns. An aggregate function is one that takes multiple values and returns a single value (like sum, avg etc). In the end you get a single row for each 'grouped by' column value.
SQL : select count(col_a) from table group_by col_b
df.groupby('col1').mean() # will group the df by col1 and set the other columns to their mean value, where applicable. It will ignore columns that it cannot apply mean to, like strings.
groubpy() will automatically convert the grouping by column to an index
df.groupby().describe() # will show the values of a whole set of functions run on df (sum, max, min etc)
df.groupby('col1').agg({'col2': np.average}) # groups by the first column and then calculates the avg value for col2.

groupby can work at dataframe and series level. At dataframe level it can take multiple columns as arguments and it will run the aggregate separately for each column. It is also possible to aggregate based on different functions for the diff cols - for eg. avg for one column and sum for another.

groupby is very powerful, useful and popular
---------------------------
Merging, joining and concatenating data frames
pd.concat([df1,df2,df3]) # will concatenate the 3 data frames along rows, meaning the result will be a data frame where the number of rows will be the sum of the number of rows of the 3 individual data frames
pd.concat([<list of dfs>], axis=1) # will concatenate along columns

pd.merge() will merge two or more data frames based on shared(key) columns
pd.merge(df1,df2) # absent any other arguments, pandas figures out the key column to join by and does an inner join
pd.merge(df1,df2, how='inner', on='key_col') # more descriptive way
pd.merge(df1,df2, how='outer', on=[<list of keys>]) # outer join based on more than one key column, the columns that are not common are put adjacent to each other with NaN where values don't exist
pd.merge(first_df, second_df, how='outer', left_index=True, right_index=True) # union merge/outer join
pd.merge(first_df, second_df, how='inner', left_index=True, right_index=True) # intersection merge/inner join
pd.merge(first_df, second_df, how='left', left_index=True, right_index=True) # left join takes all from the first that also exists in the second
pd.merge(first_df, second_df, how='right', left_index=True, right_index=True) # right join opposite of left
print(pd.merge(df1, df2, left_index=True, right_on=<shared index>))

join() merges data frames based on indexes (merge does that based on columns)
df1.join(df2)
df1.join(df2, how='outer')
---------------------------
Operations
df['col'].unique() # fetches unique values of a column
df['col'].nunique() # fetches number of unique values of a column
df['col'].value_counts() # unique values of a column and the number of times they occur (similar to R table function)

Pandas has an apply() function that works much the same way as R apply. Can apply a certain function across all elements
df['col'].apply(lambda x: x*2) # will multiply all values in col with 2.
df['col'].apply(func) # general form will apply func to all elements of col
apply() also lets you execute a function along a certain axis (row or col)
df.apply(<func>, axis=1) # apply func on columns
df['newcolumn'] = df['oldcolumn'].apply(lambda x: <do something with x>)
df['column'].apply(lambda x: x+2) # will add 2 to the value of 'column' in all the rows

sort_values() lets you sort by value
df.sort_values('col1') # sorts the df by the value of col1

pd.pivot_table() allows you to pivot your table and summarize/view data the way you want
pd.pivot_table(values='col4', index=['col1','col2'], columns=['col3']) # summarizes the values of col4, with col3 as columns and multi-indexed by col1 and col2
# a general form
df.pivot_table(values = <col which we want to summarize>, index = <col to be rows in the result>, columns = <cols of pivot table>)
df.pivot_table(values = <col which we want to summarize>, index = <col to be rows in the result>, columns = <cols of pivot table>, aggfunc = [<one or more list of func>]) 
If a list of functions are passed, say min and max, the pivot table will show two column sets for the 2 funcs.
---------------------------
Data input/output
Pandas can read CSVs, Excel, HTML and SQL databases
pd.read_csv('input.csv', index=False) # optional sep parameter if separator is not comma. index=False removes the index
pd.read_csv('file.csv', index_col=0, skiprows=1) # read a csv file, skip the first row and use the first col as index
df.to_csv('output.csv') # writes the data frame to outputfile

df = pd.read_table('file.dat', sep='<record separator>', header='<None or row#>', names='<list of names for cols>')

pd.read_excel('file.xlsx', sheetname='Sheet1')
df.to_excel('outfile.xlsx', sheet_name='newsheet') # note parameter is 'sheet_name' for to_excel and 'sheetname' for read_excel

df = pd.read_html('HTML URL')
read_html will read each HTML <table> into a separate data frame, so df is actually a list in this case and can be cycled through to get individual dfs. In many cases there will be only one table

pd.read_json('filename') # read JSON

Pandas can also read data from SQL databases after opening a connection to the database using sqlalchemy
from sqlalchemy import create_engine
engine = create_engine('sqlite:///:memory:') # generic ('<dbtype>:///<db name>')
df.to_sql('my_table', engine) # create a new table called my_table in the database
pd.read_sql('my_table', con=engine) # read a db table into a data frame, with connection being the engine
---------------------------
Pandas idioms
Index chaining is bad - back to back square brackets as in df.loc[row][column]
Method chaining is recommended - for eg.
(df.where(column == value)
    .dropna()
    .setindex(....)
    .rename(...))
if you begin a statement with (, in python, you can span a statement over mutliple lines.
-----------------------------
Scales - 4 main categories
Ratio (mathematical operations like + and * are valid)
Interval (equally spaced units)
Ordinal (shows some type of order)
Nominal (no order)

df[col].astype('category') will transform that col into the category scale if the indexes refer to categories
astype('category', categories=<list of values>, ordered=True) # ordered = True makes the categories ordinal

Once converted to categorical, a df can be queried based on the category. For eg. >= 'C' if the categories are A,B,C,D and ordered.

pd.cut - divides a column into a specified number of bins
-----------------------------
Time functions
pd.Timestamp() # converts an input like 10/12/2017 to a standard yyyy/mm/dd format
pd.Period() # converts input into a period eg. 01/2017 will be converted to a month period and 01/01/2017 to day

DateTimeIndex, PerodIndex - refers to indexes that are DateTime(Timestamp) or Period

pd.to_datetime() # converts input into Timestamp

Timestamps can be added to or subtracted from each other and it gives a Timedelta type. Similarly adding or subtracting Timestamp and Timedelta gives Timestamp. Timedelta is a certain interval of time eg. 12D or 13H

pd.date_range() # creates a date range just like a sequence with intervals etc.
-----------------------------
Tricks
# create and populate a new column based on values in another column stored as a dict
dict = {'exist_col_val1':'new_col_val1', 'exist_col_val2':'new_col_val2', 'exist_col_val3':'new_col_val3'}
df['new_column'] = df['exist_column'].map(dict) # creates a new column filling in with values from dictionary

# drop rows with duplicated values in a column, keeping the first
df.drop_duplicates(subset='<column name', keep='first')

# find 'difference' between 2 data frames, one of which is fully included in the other
pd.concat([fulldf,partdf]).drop_duplicates(keep=False)

# to convert a list of dictionaries, usually generated from a json file, into a dataframe where the json keys are fields
pd.DataFrame(list)

pd.concat([Series(row['var2'], row['var1'].split(',')) for _, row in a.iterrows()]).reset_index()

pd.concat([Series(row['ComplexID'], row['ComplexName'], row['subunits(Entrez IDs)'].split(';')) for _, row in a.iterrows()]).reset_index()

-----------------------------
Summary
create
 - pd.Series(<list>) # converts list to Series, default numerical index (row names)
 - pd.Series(<list>, index=<list with custom indexes>)
 - pd.DataFrame([<list1>,<list2>,<list3>], columns=<list with column names>) # each <list> should be [..]
 - pd.DataFrame(<dict>) # column names will be dict keys
 - pd.DataFrame(<dict>, columns=<list with names>)

append/concat
 - df1.append(df2) # adds contents of df2 to df1
 - pd.concat([df1, df2, df3]) # concatenates 3 dfs into one

merge
 - pd.merge(df1, df2, on='<column or list of columns>', how='<inner/outer etc. inner being default>')

melt
 - pd.melt(df) # unpivots df into 'variables' and 'values', variables being the column names and values being corresponding values In essence columns are stacked on top of each other into a 2 column df of vars and vals
 - pd.melt(df, id_vars=<col name>) # will make the column a third column and the var and vals will only be the remaining cols

pivot
 - df.pivot(index='<column1>', columns='<column2>', values='<column3>') # creates a stacked df with column1 as index/rows, column2 as columns and column3 as the values
 - df.pivot_table # when there are numerical values to aggregate

summarizing
 - type(df) # will return DataFrame
 - df.shape # returns number of rows/columns
 - df.head()/df.tail() # as the function name suggests, optional numerical argument will return that many rows from the head/tail
 - df.index # gives row names
 - df.columns # gives column names
 - df.dtypes # data type of each column
 - df.values # returns underlying numpy array
 - df.info() # concise summary

selecting data
 - df['column'] or df.column # select the column
 - df.loc[rowname, colname] # fetches value at that row/column intersection. 'loc' helps fetch based on row/col name
 - df.loc[:, colname] # all rows for colname, same logic df.loc[rowname,:]. 'loc' should always specify both row/col
 - df.iloc[0] # first row, all cols. 'iloc' is by index, using the numerical index for rows/cols.
 - df.iloc[0,0] # first row, first col. Note that you don't need both row and col specs like loc

filtering data
 - df[df.column == '<some value>'] # selects all rows where column matches value
 - df[df.column1 == '<some value>'].column2 # selects a column from the filtered results

sorting data
 - df.sort_values(by='<colname>') # sorts df based on values of the column passed
 - df.sort_values(by=['col1','col2']) # sorts by col1, then col2
 - df.col1.sort_values() # sorts the values of col1 and returns as a Series

descriptive statistics
 - df.describe() # summarizes all the numeric columns - showing mean, std, min, max etc.
 - df.describe(include='all') # include all columns in the summary
 - df.groupby('<col>').mean()) # groups by a specific column and sets other columns to their mean.

quality check
 - df.duplicated() # returns a boolean array for all rows, duplicated rows will be 'True'
 - df.duplicated().sum() # returns the number of duplicates
 - df.drop_duplicates() # drops duplicated rows

missing data
 - df.isnull() # returns dataframe of booleans based on if data is missing from any of the rows/cols
 - df.dropna() # drops row if any values are missing
 - df.dropna(how='all') # drops row only if all values are missing
 - df.ix[df.col1.isnull(), 'col1'] = df.col1.mean() # fill missing values of col1 with mean of the column

rename values
 - df.col1.map({'val1':'modval1', 'val2':'modval2'}) # map changes the values using a dict, val1 becomes modval1 etc.

file I/O
 - pd.read_csv('<filename.csv>') # reads a csv
 - df.to_csv('<filename.csv>') # writes df out to csv

database I/O
 - engine = create_engine('sqlite:///:memory:') # generic ('<dbtype>:///<db name>')
 - df.to_sql('my_table', engine) # create a new table called my_table in the database
 - pd.read_sql('my_table', con=engine) # read a db table into a data frame, with connection being the engine
 - pd.read_sql_query("select * from table", engine) # executes sql query on db