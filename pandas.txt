import pandas as pd

2 basic pandas data structures: Series and DataFrames
Series: one dimensional array, somewhere between a list and a dictionary. Like lists, except they have an associated array of data labels instead of just index locations. Single indexed column of data. Items are stored in an order and there are labels with which they can be retrieved.

list a = [1,2,3]
x=pd.series(list, index=['First','Second','Third']) # stores a list of 3 numbers and labels them, so they can be retrieved either by x[index] or x.label.

underneath, pandas stores data in a numpy typed array. Numbers are stored by default as float64, strings as object.
Dictionaries can also be converted to pandas Series, no need to label.

a = pd.Series([1,2,3,4]) # converts a list to a series
b = pd.Series([1,2,3,4], index=['a','b','c','d']) # creates the same series, but now with custom index labels
a.values # prints the values of the Series
b.index # prints the indexes
b[['c','a','d']] # returns correponding values
Numpy operations like filtering with boolean array, scalar multiplication, math functions etc. apply to Series as well.
Dicts can be converted to Series as:
Series(<dict variable>) # where dict var is {'key1': 'val1', 'key2':'val2' etc}
Series with the same indexes can be aligned (values summed)
A Series' index can be altered in place by assignment to object.index eg. a.index = ['w', 'x', 'y', 'z'] 

a.isnull() will show as True any keys that have no value

Vectorization: do an operation on all elements of a data structure without looping.
np.sum(a) # will calculate the sum of all elements in the series a without looping through them
much faster than looping
###################################
DataFrame: Tabular, spreadsheet like data structure containing an ordered collection of columns each of which can be a different value type (numeric, string, bool etc)

In general, you could say that the Pandas data frame consists of three main components: the data, the index, and the columns.

A dict of equal length lists can be converted to a DataFrame
data = {'state': ['OH','OH', 'OH', 'NV', 'NV'], 
	'year':[2000,2001,2002,2001,2002], 
	'pop': [1.5,1.7,3.6,2.4,2.9]}

df = DataFrame(data)
df = DataFrame(data, columns=[<list of data columns in order you want>])
df.index =[<list of row names you want>] # will name the rows
# all in one shot
df  = pd.DataFrame(data, columns=[<list of col names>], index=[<list of row names>])

Columns can be pulled out either as df['column'] or df.column
df.columns gives the column names and types
Rows can be selected by df.ix[<rownum>]
Rows can be named with the 'index' function:
df.index=['a','b','c','d'] will give these letter as rownames and can be retrived by df.ix['a'] etc.
df.iloc[index] # used to retrieve by index, same as df[index] or df.ix[index]
df.loc['label'] # used to retrieve by label, same as df['label']

A data frame can be formed by combining a set of equal length pd.Series, ndarrays, lists or dicts of lists (in which the keys will automatically be the column names as above
df = pd.DataFrame([series1, series2, series3])
df2 = df.append(<list>) # will create y adding a new list to df, but won't change df

#Access data based on labels (row/column names)
df.loc[rowname/id, colname] # will pull out the value in that cell, same as R data.frame
df.loc[:,[col1, col2]] # pulls out all rows for the 2 cols mentioned

# To slice a chunk (some number of rows for a certain set of columns)
df.loc[fromrowid:torowid, fromcolumn:tocolumn]
# to get all rows for a select set of columns, use a single colon for rows
df.loc[:, 'fromcolumn':'tocolumn']

# copy/delete columns
df.drop(row) # won't drop the row from the original df
df.drop(row, inplace=True) # will drop from original
df.drop(df[index], inplace=True) # better general syntax
df.drop(df.index[:2], inplace=True) # drops first 2 rows
df.drop(df['column'], axis=1 inplace=True) # drops a column by name

del df[col] # deletes columnn

df.columns - gives list of columns
list(df.columns) # gives the column names as a list
list(df) # short version of above

df.shape # gives number or rows and columns in a data frame

df.unstack('column') # splits a dataframe according the values of the column specified

df.T will transpose the data frame

#pandas methods to read files:
pd.read_csv('filename') # optional sep parameter if separator is not comma
pd.read_json('filename')
pd.read_excel('filename', sheetname=0) # to read the first sheet in case there are multiple sheets
#An url can also be passed as argument to the above functions

df = pd.read_csv('file.csv', index_col=0, skiprows=1) # read a csv file, skip the first row and use the first col as index
df = pd.read_table('file.dat', sep='<record separator>', header='<None or row#>', names='<list of names for cols>')

# rename columns 2 ways
df.columns=[<list of new column names>]
df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)

#To index with a specific column
df.set_index('colname') 
# above does not change the dataframe, just spits out a modified df. To actually change the df:
df = df.set_index('column') #or
df.set_index('column', inplace=TRUE)
df.set_index([list of cols]) # index with multiple columns
df = df.reset_index() # will reset index

#Access data based on indexes/positions
df.iloc[1:4, 3:5] # fetches rows 1,2 and 3 of column numbers 3&4

#For access with a mix of labels and positions, use ix
df.ix[1:4, 'fromcolumn':'tocolumn']

#To delete a row or column from data frame
df.drop('colname', 1, inplace=True) # 1 means column 
df.drop('rowname', 0, inplace=True) # 0 means row
#To delete by index instead of names/labels
df.drop(df.index[from:to, 0] # change to 1 for columns

#To add a column to a data frame
df['newcolumn'] = "value" # value will be propagated through all the rows of the data frame
# if the values are different, they need to be a list

#To add a new row, transpose the df, add a column and then transpose back

#To apply a function to all rows of a column without looping
df['column'].apply(functionname)
#function can also be a lambda
df['newcolumn'] = df['oldcolumn'].apply(lambda x: <do something with x>)
For eg.
df['column'].apply(lambda x: x+2) # will add 2 to the value of 'column' in all the rows

Querying a DataFrame:
By creating boolean arrays
df[col] > 0 # will create a boolean array where every entry that satisfies the condition will be true and the rest false. This array can be super-imposed on the dataframe to query with the 'where' clause:

df.where(df[col] > 0) # same as in R
df[df[col]>0] # same as above

df[col].unique() # unique values from a col
df = df[df[col] == <some value>] # recreates the data frame with only those entries that satisfy the condition
df = df[<list of cols to keep>] # edits dataframe to drop all other cols

Missing values:
df.fillna(method = 'ffill')

-----------------------------
Merging data frames:

Two merge 2 different data frames which share an index. For eg. a df of staff and students can be merged if the 'name' is a shared index.
pd.merge is the function that does all the merging

pd.merge(first_df, second_df, how='outer', left_index=True, right_index=True) # union merge/outer join
the columns that are not common are put adjacent to each other with NaN where values don't exist.

pd.merge(first_df, second_df, how='inner', left_index=True, right_index=True) # intersection merge/inner join
pd.merge(first_df, second_df, how='left', left_index=True, right_index=True) # left join takes all from the first that also exists in the second
pd.merge(first_df, second_df, how='right', left_index=True, right_index=True) # right join opposite of left
print(pd.merge(df1, df2, left_index=True, right_on=<shared index>))
-----------------------------
Pandas idioms
Index chaining is bad - back to back square brackets as in df.loc[row][column]
Method chaining is recommended - for eg.
(df.where(column == value)
    .dropna()
    .setindex(....)
    .rename(...))
if you begin a statement with (, in python, you can span a statement over mutliple lines.

Pandas has an apply() function that works much the same way as R apply.
apply() lets you execute a function along a certain axis (row or col)
df.apply(<func>, axis=1) # func could be a lambda
-----------------------------
groupby
when you have a data frame and want to group by a certain column, you can either loop through the data frame or use groupby (much faster)

while groupby(col) is standard, groupby can also take a function arugument. The function could be something that categorizes based on some complex criteria (not just col name)

The output from groupby can be passed on to the agg (aggregate) function to aggregate based on the result of the grouping

df.groupby('col1').agg({'col2': np.average}) # groups by the first column and then calculates the avg value for col2.

groupby can work at dataframe and series level. At dataframe level it can take multiple columns as arguments and it will run the aggregate separately for each column. It is also possible to aggregate based on different functions for the diff cols - for eg. avg for one column and sum for another.

groupby is very powerful, useful and popular
-----------------------------
Scales - 4 main categories
Ratio (mathematical operations like + and * are valid)
Interval (equally spaced units)
Ordinal (shows some type of order)
Nominal (no order)

df[col].astype('category') will transform that col into the category scale if the indexes refer to categories
astype('category', categories=<list of values>, ordered=True) # ordered = True makes the categories ordinal

Once converted to categorical, a df can be queried based on the category. For eg. >= 'C' if the categories are A,B,C,D and ordered.

pd.cut - divides a column into a specified number of bins
-----------------------------
pd.pivot_table() allows you to pivot your table and summarize/view data the way you want

df.pivot_table(values = <col which we want to summarize>, index = <col to be rows in the result>, columns = <cols of pivot table>, aggfunc = [<one or more list of func]) 

If a list of functions are passed, say min and max, the pivot table will show two column sets for the 2 funcs.

Time functions
pd.Timestamp() # converts an input like 10/12/2017 to a standard yyyy/mm/dd format
pd.Period() # converts input into a period eg. 01/2017 will be converted to a month period and 01/01/2017 to day

DateTimeIndex, PerodIndex - refers to indexes that are DateTime(Timestamp) or Period

pd.to_datetime() # converts input into Timestamp

Timestamps can be added to or subtracted from each other and it gives a Timedelta type. Similarly adding or subtracting Timestamp and Timedelta gives Timestamp. Timedelta is a certain interval of time eg. 12D or 13H

pd.date_range() # creates a date range just like a sequence with intervals etc.
-----------------------------