Learning properties from samples of available data and then try to predict properties of unknown data. Samples could have single (univariate) or multiple features/properties (multivariate).

Supervised learning: Training data comes with the properties we want to predict. A couple of main types -
1) Classification: Training data belongs to two or more classes and are labeled so. Using that info try to predict what classes sample data belongs to. eg. hand written digit recognition.
2) Regression: Predict a feature/features as a function of other features. Eg.length of a fish as a function of its age and weight.

Unsupervised learning: Training data does not come with attributes. Instead find patterns/groups within it based solely on the data.
1) Clustering
 (discovering groups)
2) Density estimation (distribution of data with the input space)
3) Dimensionality reduction (for the purpose of visualization)

The core idea is to train a model using training data, test against some test data for which we know the actual output and then use the model on new data
-------------------------------------------------------

Base library from ML in python - SciKit learn (sklearn)

# General syntax
from sklearn.family import Model
# for eg.
from sklearn.linear_model import LinearRegression

Steps
1. Instantiate the model/estimator
model = LinearRegression(normalize=True)
It is possible to pass all the required parameters in one call

2. Split given data into training and test
from sklearn.model_selection import train_test_split
# Suppose you have data that has X (features) and y (labels) it can be split as follows:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # this means use 30% of data to create test, rest train
# the above is for supervised learning. For unsupervised, you only have data, no x/y values (no labels)

3. Fit your data to the model (means calculate parameters that best describe your data)
model.fit(X_train,y_train)

4. Test using testing data (means predict labels - y values - for the test data) 
predictions = model.predict(X_test) 

5. Evaluate predictions
model.score() and model.predict_proba() will show you how close the predictions are to real values

Unsupervised models also come with model.transform() which will transform new data into new basis.
-------------------------------------------------------
Linear regression:
Regression in general means regress (move towards) the average. In linear regression we draw a line that fits the data best, in other words a line that minimizes the squared distances between the actual values and the line

When you have a dataset, for eg. as a csv read into a pandas df
The first step is to split the predictor and predicted variables into two dfs
X = df[<list of predictor variables>]
y = df[<variable to be predicted>] # usually one, like Price of a house

Second step is to create the training and test data using train_test_data from sklearn.model_selection
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, <optional random state = any number>)

Third step is to train a model with the training data
from sklearn.linear_model import LinearRegression
# create an object for the model
lm = LinearRegression()
# fit the object to training data
lm.fit(X_train, y_train) # the lm object now has the coefficients and the y-intercepts of the predictor variables

Fourth step is to use the model and predict the test set and then compare to actual test values
predictions=lm.predict(X_test)
plt.scatter(predictions, y_test) # visual comparison if the points are more or less along a straight line

3 main regression evaluation metrics
Absolute mean error - mean of the differences between predicted and actual values (called residuals)
Mean squared error - mean of the squares of the residuals
Root mean squared error - root of the above

from sklearn import metrics
metrics.mean_absolute_error(y_test, predictions)
metrics.mean_squared_error(y_test, predictions)
np.sqrt(metrics.mean_squared_error(y_test, predictions)) # there is no separate function, just have to use np.sqrt

Residuals - difference between actual observed values and predicted ones - give a good idea of how good the model is. Plotting the residuals is the way to find out
sns.distplot(y_test - predictions, bins=<some value>)
-------------------------------------------------------
Variance/bias tradeoff - optimally the model should predict with low variance and low bias, but in practice it is a tradeoff. Depends on the training data. If we try too hard to fit the training data the model will overfit trying to accommodate the outliers and false values in the training set, resulting in errors when trying to predict test data.
-------------------------------------------------------
Logistic regression:
Similar to linear regression except that instead of predicting the value of a variable, it calculates the log odds of the variable belonging to a binary category (disease vs non-disease, book smart vs street smart, spam vs ham email). It is a method of classification (though named 'regression') - classifying into catgories.

After training the model, it can be valuated using a 'confusion' matrix - a 2,2 matrix that stores actual and predicted yes/no values. Used to label True Positives, True negatives, False positives and False negatives. These are the whole numbers predicted. Accuracy is TP+TN/Total number, Error rate is FP+FN/Total number. The matrix is structures as:
[ TP, FP
  FN, TN ]

False positives are what causes Type I errors - predicted to be true, but actually false.
False negatives cause Type II errors - predicted to be false, actually true.

First step is to fill in missing data - means on columns can be used to fill in, this process is called imputation. Then convert categorical columns to numerical by using dummy coding. Pandas provides get_dummies() for this. Then the data frame is ready for model building.

For eg. use downloadable Titanic dataset from Kaggle that contains information about the passengers and if they survived or not. Split into train and test sets with X being the various variables like age, sex, what class they were traveling in, ticket price paid etc and y being survived or not (1 or 0).

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
model.fit(X_train,y_train)
predictions = model.predict(X_test) # predictions

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,predictions)
-------------------------------------------------------
K-Nearest Neighbors (KNN)
Classification algorithm. Given a set of data points that belong to two or more different categories, predict what category a new unknown point would belong to. For this you choose a distance metric called 'k' and then classify all the points within that distance metric. After that you predict what category the unknown point belongs to depending on what category is most represented in its 'k' nearest neighbors. For eg. k=10 means you choose the 10 nearest points to the unknown point and see what category is most represented, then assign the unknown point to it.

Easy to train and predict, but not effecive when the number of features start to grow.

Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. Hence the variables need to be standardized.
/// scaling variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df.drop('TARGET CLASS',axis=1))
scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
///
Then split your dataset into training and testing and build the KNN model. Start with k=1 ie predict based on the class of 1 nearest neighbor
///
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
///
Then evaluate the predictions using classification report and/or confusion matrix
Now we need to choose an optimal k value since we can't just guess. For this we cycle through a range of k values, do predictions and append the error values (mean of difference between y_test and predicted values) to a list and then plot the list. From the plot we can see around what k value the error rate is the lowest. Pick that k value, redo predictions and evaluate. You'll see the new predictions are better than the previous ones. 
-------------------------------------------------------
Decision trees:
A variety of features determine the end result - Yes/No. Each feature is a node in the tree. Each node splits into the different values for that feature. Then there are edges where the outcomes of one feature connects to the next feature node. For eg. if weather conditions are the features, Temp can be one node and the values (hot, mild, cold) can then split into another feature, say humidity and so on.
The root node is the one feature that performs the first split. Leaves are the terminal nodes that predict the end result (yes/no). How to best split features to give the best prediction is key and is determined mathematically by entropy and information gain.
Random forests: Use multiple decision trees where the splits are done based on a random set of 'm' features out of a total of 'p' features. That means - a random feature is chosen for each split in each tree. Each tree will have a total of 'm' features, not all of 'p'. Usually 'm' is the square root of 'p'.
Why random? When using bagged trees ie using a bootstrap (sampling with replacement), if there is one feature that is a strong predictor, most trees will use that feature and will hence be highly correlated. Random ensures that doesn't happen.

from sklearn.tree import DecisionTreeClassifier
model building etc. same as before

/// to visualize tree
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot
dot_data = StringIO()  
export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)
graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())
///

Random forests
from sklearn.ensemble import RandomForestClassifier
-------------------------------------------------------
Support Vector machines:
SVM builds a model that assigns new data into one of the two classifications, given a set of training data that belongs to one of the two categories. Non-probabilistic binary linear classifier. The categories themselves are divided by a clear gap as wide as possible (meaning the data points in space are clearly divided). New examples are mapped into one of the 2 categories based on which side of the gap they fall on. The gap is called the hyperplane. To overcome the problem that there are many ways to draw a line that separates the two categories, we choose a hyperplane that maximizes the margin between the classes. So it won't be a single line itself, it will have two border lines on each side that maximizes the area of the hyperplane. The points that the margins touch on each side are called the support vectors.

This idea can also be expanded into non-linearly separable data using 'kernel' functions, viewing data in a higher (than 2) dimension. This involves adding new features and increasing the dimensionality. This enables us to separate the data non-linearly which will classify the data much better.

from sklearn.svm import SVC
model = SVC() 
# alternatively, can specify the kernel to be used SVC(kernel-'linear') - linear is also default. The kernel 'rbf' is used to classify non-linearly

Other parameters for SVM (ones you pass when you create your model/classifier)
1) C - controls tradeoff between a smooth decision boundary and classifying training points correctly, higher C means more training points classified correctly.
2) Gamma - how far the influence of a single training point reaches, low values mean far reach, high values mean close reach (no effect on linear kernels, only in rbf). In other words a high gamma would only look at the points close to the decision boundary/divding line in deciding where to draw the line. Low values means farther points are also taken into consideration.

Use parameters to stop overfitting (taking data too literally and creating a model too complex). Manually cycling through different values for parameters can be tedious. But 'GridSearch' can be used to automatically cycle through a set of parameters in a grid and pick the best combination.

# First create a dictionary of paramemeters:

param_grid = {'C': [0.1,1, 10, 100, 1000], 
	      'gamma': [1,0.1,0.01,0.001,0.0001], 
	      'kernel': ['rbf']} 

# Then import the grid search module and feed it the model and the parameter hash

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)

# fit

grid.fit(X_train,y_train)

# To find best parameters chosen
grid.best_params_

grid_predictions = grid.predict(X_test)

# calculate accuracy score
from sklearn.metrics import accuracy_score
print(accuracy_score(grid_predictions, y_test))
-------------------------------------------------------
K means clustering:
Unsupervised learning algorithm which will attempt to cluster similar groups together in the data. The data will have no labels, hence unsupervised.

Steps:
1) Choose a number of clusters K
2) Randomly assign each data point to one of the K clusters
3) Repeat the following until clusters stop changing:
	a) for each cluster calculate the cluster centroid by taking the mean of the points in the cluster
	b) move each data point to the cluster for which the centroid is the closest.

To choose a K value (number of clusters), use the elbow method. It calculates the Sum of Squares of all the points from their respective cluster centroid. When you plot K against SSE, at some value(s) of K, you will see the SSE leveling off after falling for the lower K values. Beyond that point the SSE doesn't decrease much. On the graph this gives an elbow shape, hence the name.

To test, generate some data with known number of clusters using scikit-learn

from sklearn.datasets import make_blobs
# generate 200 samples, with 2 features, belonging to 4 clusters and the std of points around a centroid is 1.8
data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.8,random_state=101)
# creates a tuple, the first element of which is a numpy array of the features and the second is the cluster it belongs to

#To visualize generated data
plt.scatter(data[0][:,0], data[0][:,1], c=data[1])
# data[0][:,0] is the xvalue which corresponds to the first column/feature on the numpy array, y is the other column and c is the cluster it belongs to. Adding a cmap will help distinguish better.

The same generated data can be used for predictions/classifications (without looking at the cluster labels attached to it.

from sklearn.cluster import KMeans
clf = KMeans(n_clusters=4) # since we already know the number of clusters, otherwise choose
kmeans.fit(data[0]) # given all the features, fit into 4 clusters
# find the centers of the clusters
kmeans.cluster_centers_
# find the labels
kmeans.labels_

Test the predicted clusters by doing a scatter plot and setting c=kmeans.labels_