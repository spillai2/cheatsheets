Learning properties from samples of available data and then try to predict properties of unknown data. Samples could have single (univariate) or multiple features/properties (multivariate).

Supervised learning: Training data comes with the properties we want to predict. A couple of main types -
1) Classification: Training data belongs to two or more classes and are labeled so. Using that info try to predict what classes sample data belongs to. eg. hand written digit recognition.
2) Regression: Predict a feature/features as a function of other features. Eg.length of a fish as a function of its age and weight.

Unsupervised learning: Training data does not come with attributes. Instead find patterns/groups within it based solely on the data.
1) Clustering
 (discovering groups)
2) Density estimation (distribution of data with the input space)
3) Dimensionality reduction (for the purpose of visualization)

The core idea is to train a model using training data, test against some test data for which we know the actual output and then use the model on new data
-------------------------------------------------------

Base library from ML in python - SciKit learn (sklearn)

# General syntax
from sklearn.family import Model
# for eg.
from sklearn.linear_model import LinearRegression

Steps
1. Instantiate the model/estimator
model = LinearRegression(normalize=True)
It is possible to pass all the required parameters in one call

2. Split given data into training and test
from sklearn.model_selection import train_test_split
# Suppose you have data that has X (features) and y (labels) it can be split as follows:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # this means use 30% of data to create test, rest train
# the above is for supervised learning. For unsupervised, you only have data, no x/y values (no labels)

3. Fit your data to the model (means calculate parameters that best describe your data)
model.fit(X_train,y_train)

4. Test using testing data (means predict labels - y values - for the test data) 
predictions = model.predict(X_test) 

5. Evaluate predictions
model.score() and model.predict_proba() will show you how close the predictions are to real values

Unsupervised models also come with model.transform() which will transform new data into new basis.
-------------------------------------------------------

Linear regression:
Regression in general means regress (move towards) the average. In linear regression we draw a line that fits the data best, in other words a line that minimizes the squared distances between the actual values and the line

When you have a dataset, for eg. as a csv read into a pandas df
The first step is to split the predictor and predicted variables into two dfs
X = df[<list of predictor variables>]
y = df[<variable to be predicted>] # usually one, like Price of a house

Second step is to create the training and test data using train_test_data from sklearn.model_selection
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, <optional random state = any number>)

Third step is to train a model with the training data
from sklearn.linear_model import LinearRegression
# create an object for the model
lm = LinearRegression()
# fit the object to training data
lm.fit(X_train, y_train) # the lm object now has the coefficients and the y-intercepts of the predictor variables

Fourth step is to use the model and predict the test set and then compare to actual test values
predictions=lm.predict(X_test)
plt.scatter(predictions, y_test) # visual comparison if the points are more or less along a straight line

3 main regression evaluation metrics
Absolute mean error - mean of the differences between predicted and actual values (called residuals)
Mean squared error - mean of the squares of the residuals
Root mean squared error - root of the above

from sklearn import metrics
metrics.mean_absolute_error(y_test, predictions)
metrics.mean_squared_error(y_test, predictions)
np.sqrt(metrics.mean_squared_error(y_test, predictions)) # there is no separate function, just have to use np.sqrt

Residuals - difference between actual observed values and predicted ones - give a good idea of how good the model is. Plotting the residuals is the way to find out
sns.distplot(y_test - predictions, bins=<some value>)
-------------------------------------------------------

Variance/bias tradeoff - optimally the model should predict with low variance and low bias, but in practice it is a tradeoff. Depends on the training data. If we try too hard to fit the training data the model will overfit trying to accommodate the outliers and false values in the training set, resulting in errors when trying to predict test data. A heavy 'bias' model won't learn anything new. A high variance model is the opposite that will react to everything it has learned before, but won't know what to do when confronted with something it hasn't learned before.
-------------------------------------------------------

Logistic regression:
Similar to linear regression except that instead of predicting the value of a variable, it calculates the log odds of the variable belonging to a binary category (disease vs non-disease, book smart vs street smart, spam vs ham email). It is a method of classification (though named 'regression') - classifying into catgories.

After training the model, it can be valuated using a 'confusion' matrix - a 2,2 matrix that stores actual and predicted yes/no values. Used to label True Positives, True negatives, False positives and False negatives. These are the whole numbers predicted. Accuracy is TP+TN/Total number, Error rate is FP+FN/Total number. The matrix is structures as:
[ TP, FP
  FN, TN ]

False positives are what causes Type I errors - predicted to be true, but actually false.
False negatives cause Type II errors - predicted to be false, actually true.

First step is to fill in missing data - means on columns can be used to fill in, this process is called imputation. Then convert categorical columns to numerical by using dummy coding. Pandas provides get_dummies() for this. Then the data frame is ready for model building.

For eg. use downloadable Titanic dataset from Kaggle that contains information about the passengers and if they survived or not. Split into train and test sets with X being the various variables like age, sex, what class they were traveling in, ticket price paid etc and y being survived or not (1 or 0).

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
model.fit(X_train,y_train)
predictions = model.predict(X_test) # predictions

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,predictions)
-------------------------------------------------------

Naive Bayes
Conditional probability (Bayes rule) based binary classification of data. Each observation/data point is assigned to its most likely class given its predictor variables. If the probability is > 0.5 of being Yes/1, then it is assigned to Yes, otherwise to No/0. For the entire data set, it draws a decision boundary that separates the two classes. On this boundary, the probability of belonging to either class is 0.5.

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
clf = GaussianNB()
clf.fit(features_train, labels_train)
pred = clf.predict(features_test)

Bayes error rate is the lowest of all algorithms. In theory this is the best classification algorithm, but in practice computing it is impossible in most cases because we do not know the conditional distribution of the prediction given the predictors. Also it assumes the features are independent which is seldom the case in real-world data. So it serves as an unattainable gold standard against which to compare other methods.
-------------------------------------------------------

1-Nearest neighbor
Classification of data based on euclidean distance in vector space. For eg. classification of hand written digits. A set of training data will have a large number of hand-written digits and associated labels. Training on this data, the algorithm outputs a model (classifier) that can predict a newly supplied input associating it with one of the labels from the training data. This is done by calculating the euclidean distance between points in the 2 vector spaces - one of the input data and one of the training data. In the case of an image of 784 pixels (28x28) the vector is a concatenation of the 28 rows  (each pixel is an element of such a vector) and the distnace is calculated between corresponding pixels of the input and training data. The distances are summed up and the label that correponds to the image that has the lowest distances (nearest neighbor) is output as the prediction.

Euclidean distance of a vector is denoted by ||V|| and is the square root of the sum of the squares of the sides of a right angle triangle that joins the two points. That is in case of a two dimensional space. In case of 784 dimensions, we take the sqrt of the sum of the squared sum of the difference between the two vectors.

K-Nearest Neighbors (KNN)
Classification algorithm. Given a set of data points that belong to two or more different categories, predict what category a new unknown point would belong to. For this you choose a distance metric called 'k' and then classify all the points within that distance metric. After that you predict what category the unknown point belongs to depending on what category is most represented in its 'k' nearest neighbors. For eg. k=10 means you choose the 10 nearest points to the unknown point and see what category is most represented, then assign the unknown point to it.

Easy to train and predict, but not effecive when the number of features start to grow.

Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. Hence the variables need to be standardized.
/// scaling variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df.drop('TARGET CLASS',axis=1))
scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
///
Then split your dataset into training and testing and build the KNN model. Start with k=1 ie predict based on the class of 1 nearest neighbor
///
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
///
Then evaluate the predictions using classification report and/or confusion matrix
Now we need to choose an optimal k value since we can't just guess. For this we cycle through a range of k values, do predictions and append the error values (mean of difference between y_test and predicted values) to a list and then plot the list. From the plot we can see around what k value the error rate is the lowest. Pick that k value, redo predictions and evaluate. You'll see the new predictions are better than the previous ones. 
-------------------------------------------------------

Decision trees:
An algorithm that works by asking multiple questions on features in a sequential manner with the next question in the sequence depending on the answer to the previous question. The answers are usually yes/no, features are the questions. A variety of features determine the end result. Each feature is a node in the tree. Each node splits into the different values for that feature. Then there are edges where the outcomes of one feature connects to the next feature node. For eg. if weather conditions are the features, Temp can be one node and the values (hot, mild, cold) can then split into another feature, say humidity and so on.
The root node is the one feature that performs the first split. The answer to the root node question will split the data set linearly. Further nodes/questions will split the selected data linearly until you arrive at an answer. Leaves are the terminal nodes that predict the end result. The overall data itself need not be linearly separable since each feature/question will split the data linearly at each step.

How to best split features to give the best prediction is key and is determined mathematically by entropy and information gain. 

Entropy - measure of impurity in a set of examples. The idea is to split features based on which feature gives the most pure split (as little of the wrong class in it as possible)
entropy = sum_over_i(-pi * log2(pi))
where pi is the fraction of examples in class i. A class is a type of value that a feature can take. For eg. if 'speed' is a feature, the classes could be 'slow' and 'fast'. When calculating the entropy of such a variable, you sum the logs of the fraction of examples that belong to the 'slow' class and those that belong to the 'fast' class. If the examples are evenly split between the two classes you get the max entropy of 1.0.

Entropy is 0 when the examples are all in the same class. The max value of entropy is 1 when the examples are evenly split between the available classes.

Information gain: entropy(parent) - weighted avg(entropy(children))

The goal of decision tree is to maximize information gain. During training, the algorithm tries out different features to split by and calculate the info gains and then picks the best path to split by.

///
from sklearn.tree import DecisionTreeClassifier
model building etc. same as before
///

Key parameters:
min_samples_split - minimum number of samples available to split further, default is 2. Very low min_sample_split will end up overfitting the data.

Random forests: Use multiple decision trees where the splits are done based on a random set of 'm' features out of a total of 'p' features. That means - a random feature is chosen for each split in each tree. Each tree will have a total of 'm' features, not all of 'p'. Usually 'm' is the square root of 'p'.
Why random? When using bagged trees ie using a bootstrap (sampling with replacement), if there is one feature that is a strong predictor, most trees will use that feature and will hence be highly correlated. Random ensures that doesn't happen.

/// to visualize tree
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot
dot_data = StringIO()  
export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)
graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())
///

Random forests
from sklearn.ensemble import RandomForestClassifier
-------------------------------------------------------

Support Vector machines:
SVM builds a model that assigns new data into one of the two classifications, given a set of training data that belongs to one of the two categories. Non-probabilistic binary linear classifier. The categories themselves are divided by a clear gap as wide as possible (meaning the data points in space are clearly divided). New examples are mapped into one of the 2 categories based on which side of the gap they fall on. The gap is called the hyperplane. To overcome the problem that there are many ways to draw a line that separates the two categories, we choose a hyperplane that maximizes the margin between the classes. So it won't be a single line itself, it will have two border lines on each side that maximizes the area of the hyperplane. The points that the margins touch on each side are called the support vectors.

This idea can also be expanded into non-linearly separable data using 'kernel' functions, viewing data in a higher (than 2) dimension. This involves adding new features and increasing the dimensionality. This enables us to separate the data non-linearly which will classify the data much better.

from sklearn.svm import SVC
model = SVC() 
# alternatively, can specify the kernel to be used SVC(kernel-'linear') - linear is also default. The kernel 'rbf' is used to classify non-linearly

Other parameters for SVM (ones you pass when you create your model/classifier)
1) C - controls tradeoff between a smooth decision boundary and classifying training points correctly, higher C means more training points classified correctly.
2) Gamma - how far the influence of a single training point reaches, low values mean far reach, high values mean close reach (no effect on linear kernels, only in rbf). In other words a high gamma would only look at the points close to the decision boundary/divding line in deciding where to draw the line. Low values means farther points are also taken into consideration.

Use parameters to stop overfitting (taking data too literally and creating a model too complex). Manually cycling through different values for parameters can be tedious. But 'GridSearch' can be used to automatically cycle through a set of parameters in a grid and pick the best combination.

# First create a dictionary of paramemeters:

param_grid = {'C': [0.1,1, 10, 100, 1000], 
	      'gamma': [1,0.1,0.01,0.001,0.0001], 
	      'kernel': ['rbf']} 

# Then import the grid search module and feed it the model and the parameter hash

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)

# fit

grid.fit(X_train,y_train)

# To find best parameters chosen
grid.best_params_

grid_predictions = grid.predict(X_test)

# calculate accuracy score
from sklearn.metrics import accuracy_score
print(accuracy_score(grid_predictions, y_test))
-------------------------------------------------------

K means clustering:
Unsupervised learning algorithm which will attempt to cluster similar groups together in the data. The data will have no labels, hence unsupervised.

Steps:
1) Choose a number of clusters K
2) Randomly assign each data point to one of the K clusters
3) Repeat the following until clusters stop changing:
	a) for each cluster calculate the cluster centroid by taking the mean of the points in the cluster
	b) move each data point to the cluster for which the centroid is the closest.

To choose a K value (number of clusters), use the elbow method. It calculates the Sum of Squares of all the points from their respective cluster centroid. When you plot K against SSE, at some value(s) of K, you will see the SSE leveling off after falling for the lower K values. Beyond that point the SSE doesn't decrease much. On the graph this gives an elbow shape, hence the name.

To test, generate some data with known number of clusters using scikit-learn

from sklearn.datasets import make_blobs
# generate 200 samples, with 2 features, belonging to 4 clusters and the std of points around a centroid is 1.8
data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.8,random_state=101)
# creates a tuple, the first element of which is a numpy array of the features and the second is the cluster it belongs to

#To visualize generated data
plt.scatter(data[0][:,0], data[0][:,1], c=data[1])
# data[0][:,0] is the xvalue which corresponds to the first column/feature on the numpy array, y is the other column and c is the cluster it belongs to. Adding a cmap will help distinguish better.

The same generated data can be used for predictions/classifications (without looking at the cluster labels attached to it.

from sklearn.cluster import KMeans
clf = KMeans(n_clusters=4) # since we already know the number of clusters, otherwise choose
kmeans.fit(data[0]) # given all the features, fit into 4 clusters
# find the centers of the clusters
kmeans.cluster_centers_
# find the labels
kmeans.labels_

Test the predicted clusters by doing a scatter plot and setting c=kmeans.labels_
-------------------------------------------------------

Principal Component Analysis (PCA)
Unsupervised learning algorithm, also known as general factor analysis. Basically a transformation of the data to find out what features explain the most variation.

Used to stuy the interrelationships between variables to understand the underlying structures. While regression fits a line to the data, PCA fits several orthogonal (perpendicular) lines that together explain most/all of the variance in the data set. The goal is to understand the factors/variables that contribute to most of the variance in the data. This is achieved by understanding 'components' which are linear transformations that chooses a variable system such that the greatest variance in the data set lies on the first axis, second greatest on the second axis etc. This process allows us to reduce the number of variables used in an analysis - dimensionality reduction (dimensions = variables).

The components do not correspond one-to-one with features, the components are actually combinations of features. The idea is to reduce a high-dimensional data set to 2 or 3 components and feed that into an ML algorithm to classify. 

First step is to scale the data so that each feature has single unit variance

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df) # where df is the dataframe with data that we want to scale
scaled_data = scaler.transform(df) # transform

# now do PCA on the scaled data
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data) # transform to 2 principal components

At this point x_pca has the same data as scaled_data, but condensed into 2 features/components. The transformed data can now be plotted with the two components as the 2 axis - first PC along the x-axis and second along the y-axis. It is not easy to interpret the 2 components in terms of the features it condensed. For that we can create a heatmap of the components against the features. That will tell correlation exists between individual features and each of the two components. 

In an example cancer dataset with 30 odd features deciding if a tumor is malignant or benign, reducing down to 2 dimensions we can see that some features are strongly correlated to the malignant tumor than to the benign one.
-------------------------------------------------------

Recommender systems
Makes recommendations based on available ratings/features. Use pandas corrwith() function for this. First pull out the ratings of a given entity, lets say a movie, from the larger data set that contains user ratings for all entities/movies. Then:

<master data frame>.corrwith(<ratings of individual movie>)

The output will be a pandas series by default, can be converted to a dataframe for ease of use:

df_with_correlations = pd.DataFrame(<master data frame>.corrwith(<ratings of individual movie>), columns=['Correlation'])

From the data frame created this way, it is easy to sort the correlations and pick the top few that are closely correlated to the movie in question.
-------------------------------------------------------

Natural Language Processing (NLP)
Extract information from text. Features are words. A collection of features is assembled and then a corresponding count is calculated for each feature/word for each document. A document represented as a vector of word counts is called a 'bag of words'.

Once documents are represented as bags of words, we can use cosine similarity to compare the vectors to see how closely related the documents are. The bag of words can be improved upon by adjusting word counts based on their frequency in the corpus (group of documents)

Term frequency: importance of the term within a document
tf(d,t) = number of times term t appears in document d
Inverse document frequency (IDF): importance of the term in the corpus itself
idf(t) = log(D/t), where D is the total number of documents and t is the number of documents with the term

TF-IDF can then be expressed as:
tf(x,y) * log (N/df(x))
where tf(x,y) is the term frequency of term x in document y and df(x) is the number of documents that contain the term x and N is the total number of documents.
tf(x,y) - number of times x appears in y divided by the total number of

Steps in text processing:
1) Remove punctuation - use string.punctuation
2) Tokenization - remove stop words and collect only words we want
3) Vectorization - convert the tokens (lists of words) to vectors that can be used for ML. Convert a set of documents into a matrix where words are the rows, the document names/ids are the columns and the counts of the word are values.

Many words will have zero counts in most documents, so scikit-learn returns a sparse matrix (most values are zero)

from sklearn.feature_extraction.text import CountVectorizer
bag_of_words = CountVectorizer(analyzer=<a function that does the above 3 steps>)
bag_of_words.fit(df['text'])

bag_of_words.vocabulary_ stores the list of words identified

bag_of_words.transform([<document in list format>]) # matches each word in the list with the words in the vocabulary and assigns counts

tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer().fit(<bag_of_words_transformed_data>)
tfidf_transformer.transform(<bag_of_words_transformed_data>)

messages_tfidf = tfidf_transformer.idf_[<index of word in transformed bow>]

Train a model:
from sklearn.naive_bayes import MultinomialNB # or any other NB
trained_model = MultinomialNB().fit(messages_tfidf, messages['label'])

Test trained model by passing a known 'label' and see it the prediction is correct. Full model evaluation:

predictions = trained_model.predict(messages_tfidf)
-------------------------------------------------------

Neural networks and Deep learning
Neural networks are modeled after biological neural networks and attempt to allow computers to learn in a similar manner to humans - reinforcement learning. Use cases: Pattern recognition, Time Series Predictions, Signal processing, Anomaly detection, Control.

The human brain has interconnected neurons with dendrites that receive inputs and then based on those inputs produce an electrical signal output through the axon which is transmitted to the adjacent neuron.

Neural networks attempt to solve problems that would normally be easy for humans but hard for computers (like recognizing a person from a picture). The key feature of NN when compared to other algorithms is that it also takes into account the interactions between the different parameters (where as algorithms like linear regression take each parameter separately and do not consider how they could be interactiing with each other). The input to an NN is called the input layer and the final output is the output layer. In between are hidden layers (one or more) that are interactions between the different nodes in the input layer. The whole system with all the layers is called a network (just like network of neurons).

For eg. lets say the goal is to calculate how many transactions a customer would make in a year at a bank based on input data that includes the persons age, number of children and number of accounts. Note that each of these parameters could be interacting with each other, so a straight prediction will not give an accurate result. The parameters form the nodes of the input layer. The 'forward propagation' algorithm will pass this information through the network to make a prediction. Lets say we consider only the number of children and accounts. A given customer has 2 children and 3 accounts. Lines connect the input to the hidden layer. Each line has associated with it a 'weight' that indicates how strongly that input affects the hidden node. The value at each node is the sum of the (weight * value) of each input node that is connected to it. The hidden layer is then connected to the output node by lines which also come with weights and follow the same protocol for calculation. The multiply-add process is similar to the dot product in lin algebra (product of two matrices).

The dot product of input and weights is only half of the story. To get to the full power of NNs, there is an 'activation function' that is applied to the dot product at each node to give the node's value. This function calculates non-linear relationships in the interactions - for eg. how does going from 2 children to 3 change behavior as opposed to going from 3 to 4. The accepted activation function is ReLU (Rectified Linear Unit) function which takes a single number input and returns the number if positive and 0 if negative.

Deep networks build patterns of representations of the data which gets increasingly complex as it goes through the layers. This partiallyb replaces the need for feature engineering which extracts the most useful features.

Deep learning - use of many successive hidden layers (as opposed to one in classic NNs) resulting in a 'deeper' network. Used to be 5-10-15 layers but can now go into the thousands.

Simplest neural network possible: the perceptron. It consists of one or more inputs, a processor and a single output. It follows the feed forward model meaning inputs are fed into the neuron, are processed and results go to the output. It has 4 main steps:
1) Receive inputs
2) Weight inputs
3) Sum inputs
4) Generate output

Each input has a weight associated with it (between -1 and 1) and it is the weighted input that is received at the processor. The processor sums them up and sends it to an activation function which tells the neuron to fire or not. In case of a simple binary function the output is either -1 or 1. The job of the activation function is to introduce non-linearity into the output of the node (since most real world data is non-linear, it would help to train the neuron on non-linear data)

The activation function can be logistic, sigmoid, trignometric, step etc. 

To prevent a 0 output, an additional input of 1 with an associated weight called bias is added to the sum. The main function of Bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). Akin to the y-intercept in the equation for a line.

To train a perceptron the following steps are followed:
1) Provide the perceptron with inputs for which there is a known answer.
2) Ask the perceptron to guess the answer.
3) Compute the error - how far off is the answer from the real answer.
4) Adjust all weights according to the error
5) Return to step 1 and repeat.

This layers a set of perceptron one after the other until we arrive at the correct answer and that is what creates a 'neural net'. This is how a single perceptron would work. To create a neural network, link many perceptrons/nets together.

Neural networks have an input layer and an output layer. Any layers in between are known as hidden layers because you don't see anything directly except the input and output layers.

Deep learning refers to a neural network with many hidden layers, causing it to be deep. For eg. Microsoft's state of the art vision recognition uses 152 layers.
-------------------------------------------------------

TensorFlow
One of the most popular deep learning libraries, developed by Google. It can run on either the CPU or GPU. In general, deep neural networks run faster in GPU. The basic idea is to be able to create data flow graphs which have nodes and edges. The data passed from a layer of nodes to another is called the Tensor.

Two ways to use TensorFlow:
1) Customizable Graphics session
2) SciKit-learn type interface with Contrib.Learn

import tensorflow as tf
# tf constants
x = tf.constant(10) # creates a tensor object, in this case a constant integer

# tf sessions
sess = tf.session() # creates a 'session' object to execute operation objects
sess.run(x) # will print out 10

# tf operations
x = tf.constant(2)
y = tf.constant(3)
with tf.Session() as sess:
    print('Addition',sess.run(x+y))

# tf placeholders for future use - for eg. for a 1024 x 1024 matrix
x = tf.placeholder(tf.float32, shape=(1024, 1024))
# or for an operation 
x = tf.placeholder(tf.int32)
y = tf.placeholder(tf.int32)

# to multiply 2 matrices
1. Create the matrices in tf
mat1 = tf.constant(np.array([[5.0,5.0]])) # create a 1x2 array
mat2 = tf.constant(np.array([[2.0],[2.0]])) # create a 2x1 array
2. Create the matrix multiplication object
matrix_multi = tf.matmul(mat1,mat2)
3. Execute the operation in a session
with tf.Session() as sess:
    result = sess.run(matrix_multi)
    print(result)

# MNIST example - classic dataset of handwritten numbers
# consists of images of 55000 handwritten digits and corresponding labels. The goal is to iterate through the images and adjust the error/weights until we start to get the best predictions. Then predict test images.

# Import MINST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True) # downloads the data into /tmp/data

Data Format
The data is stored in a vector format, although the original data was a 2-dimensional matrix with values representing how much pigment was at a certain location. Values range from 0 (white) to 1 (black). Each vector is an image with 784 values, there are 55000 such vectors. The individual vectors can be 'reshaped' into 28x28 matrices, which are numpy arrays holding values of the individual images.

sample = mnist.train.images[2].reshape(28,28) # fetches a single image as a numpy array
plt.imshow(sample, cmap='gray') # display image in grayscale

Parameters
Need to define 4 parameters, it is really hard to know what good parameter values are on a data set for which you have no experience with, however since MNIST is pretty famous, we have some reasonable values. The parameters here are:

Learning Rate - How quickly to adjust the cost function.
Training Epochs - How many training cycles to go through (how many times to check for error and rerun)
Batch Size - Size of the 'batches' of training data

# Network Parameters
n_hidden_1 - num features/nodes in first layer
n_hidden_2 - num features/nodes in second layer
n_input = number of values in an input (single image - 784 or 28*28)
n_classes = 10 # MNIST total classes (0-9 digits)
n_samples = mnist.train.num_examples (total size of the training data 55000 images in this case)

MultiLayer Model
First we receive the input data array and then to send it to the first hidden layer. Then the data will begin to have a weight attached to it between layers (this is initially a random value) and then sent to a node to undergo an activation function along with a Bias. Then it will continue on to the next hidden layer, and so on until the final output layer. In our case, we will just use two hidden layers, the more you use the longer the model will take to run (but it has more of an opportunity to possibly be more accurate on the training data).

Once the transformed "data" has reached the output layer we need to evaluate it. Here we will use a loss function (also called a cost function) to evaluate how far off we are from the desired result. In this case, how many of the classes we got correct.

Then we will apply an optimization function to minimize the cost (lower the error). This is done by adjusting weight values accordingly across the network. We can adjust how quickly to apply this optimization by changing our earlier learning rate parameter. The lower the rate the higher the possibility for accurate training results, but that comes at the cost of having to wait (physical time wise) for the results. Of course, after a certain point there is no benefit to lower the learning rate.
.
.
.
-------------------------------------------------------
ContribLearn - scikit-learn like interface to tensorflow

Import and do train/test split as you would do with a regular scikit-learn workflow. To train the model:

import tensorflow.contrib.learn.python.learn as learn
classifier = learn.DNNClassifier(hidden_units=[x, y, z], n_classes=<num classes>)
classifier.fit(X_train, y_train, steps=<num of iterations>, batch_size=<size of each batch of data>)
classifier.predict(X_test)

--------------------------------------------------------
ML edx course

1-Nearest neighbor
Classification of data based on euclidean distance in vector space. For eg. classification of hand written digits. A set of training data will have a large number of hand-written digits and associated labels. Training on this data, the algorithm outputs a model (classifier) that can predict a newly supplied input associating it with one of the labels from the training data. This is done by calculating the euclidean distance between points in the 2 vector spaces - one of the input data and one of the training data. In the case of an image of 784 pixels (28x28) the vector is a concatenation of the 28 rows  (each pixel is an element of such a vector) and the distnace is calculated between corresponding pixels of the input and training data. The distances are summed up and the label that correponds to the image that has the lowest distances (nearest neighbor) is output as the prediction.

Euclidean distance of a vector is denoted by ||V|| and is the square root of the sum of the squares of the sides of a right angle triangle that joins the two points. That is in case of a two dimensional space. In case of 784 dimensions, we take the sqrt of the sum of the squared sum of the difference between the two vectors.

k-nearest neighbors
To improve upon 1-nn, instead of taking just one closest neighbor we take a few (k) closest neighbors and return the label that occurs most frequently in those closest neighbors. To choose the optimum k by calculating the error rates for a range of ks, use a method called cross validation. The error rates (wrong classifications) will need to be calculated in the training data itself. For this, the training data is chunked into equal size chunks - one of the chunks is considered test data and the others are used to train the data. This is done iteratively for all the chunks ie each chunk will get to be test data at one point and training data at all other points. The error rates thus obtained are then averaged.

Called n-fold cross validation, where n is the number of chunks of the training data.

Another way to improve on 1-nn is to change the distance function. Euclidean in general is not a great distance method in some cases, for eg. image identification (say hand written digits). A slight curve or rotation gives a much higher distance than it actually is. Other methods that are more stable under small translations and rotations (eg. tangent distance) or a broader family of natural deformations (shape context). Domain knowledge can help determine the best distance method. Feature selection also helps immensely - from the available data, choose only features that are relevant for classification.

The speed of NN is influenced directly by the size of the training set - the larger they are, the longer it takes to train them. Luckily there are data structures available and included in standard python libraries, that speed up NN significantly.