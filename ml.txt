Learning properties from samples of available data and then try to predict properties of unknown data. Samples could have single (univariate) or multiple features/properties (multivariate).

Supervised learning: Training data comes with the properties we want to predict. A couple of main types -
1) Classification: Training data belongs to two or more classes and are labeled so. Using that info try to predict what classes sample data belongs to. eg. hand written digit recognition.
2) Regression: Predict a feature/features as a function of other features. Eg.length of a fish as a function of its age and weight.

Unsupervised learning: Training data does not come with attributes. Instead find patterns/groups within it based solely on the data.
1) Clustering
 (discovering groups)
2) Density estimation (distribution of data with the input space)
3) Dimensionality reduction (for the purpose of visualization)

Data split into training and testing. Learn from training data and test on testing data.

Feature scaling:
When numerical features are not on the same scale, for eg. age and salary, the larger scale - in this case salary, will greatly nullify the effect of the smaller scale variable. Feature scaling converts such features into similar scales:
2 types of feature scaling:
1) Standardization: convert to units of Standard Deviation. (x - mean_of_x)/sd_of_x
2) Normalization: (x - min_of_x)/ range_of_x , where range_of_x is max(x) - min(x)
Usually converts all values to a range between -1 and +1

Simple linear regression:
One input variable used to predict a single output variable. Equation for line:
y = b0 + b1x
b0 is the y-intercept
b1 is the slope
Simple linear regression draws all possible lines with a given data set, calculates the sum of squares of the distances of all the points from the respective lines and picks the line with the lowest SS.

Python
1) Read data set using pandas
data = pandas.read_csv(<filename>)
2) Split into X and Y values
x = data.iloc[:,<col#>].values
3) Split into test and training sets using scikit learns function for it
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<fraction needed>, random_state=0)
4) Create a linear regression object from scikit learn's linear_model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression() # this object is the machine we are creating
5) Train the model/object with training data
regressor.fit(x_train, y_train) # in this step, the machine learns and creates an eqn
6) Test on the test data
y_pred = regressor.predict(x_test)
7) Visualizing results using matplotlib's pyplot module
plt.scatter(X_train, y_train, color='red') # plotting actual/observed data points
plt.plot(X_train, regressor.predict(X_train), color='blue') # draw the regression line

Multiple linear regression
Multiple predictor variables - one prediction 
y = b0 + b1x1 + b2x2...+bnxn
eg. predicting profit generated by a company based on R&D Spending, Admin, Marketing etc.
Dummy variables
When predictor variables are categorical (not numerical) we add extra columns for each of the values and then assign values of 1 or 0 based on if the given row has that value for the variable in question. These extra columns (values of the categorical variable) are called dummy variables. In the equation, we no longer use the categorical variable itself, but string on the extra dummy variables that we created for its values. 

eg. profit prediction mentioned above could have a State variable which denotes the state the company is located in. So we create as many dummy variables as there are states in the data.

You have to exclude one of the dummy variables from each category from your model, otherwise the effect of the dummy variable is canceled out. For eg. if the dummy variables represent 2 states NY and CA, then one of them have to be dropped (the one that gets zero as the value) as that needs to be the default. Including both means we are not getting the effect of either.

Step-by-step building of an mult reg model
We do not include *all* predictor variables in the model since some may not really have any effect at all on the predicted variable. How to eliminate? There are 5 different ways of building models by eliminating vars.
1. All-in
2. Backward Elimination
3. Forward selection
4. Bi-directional elimination
5. Score comparison

2,3 & 4 are usually referred to as Stepwise Regression.

1. is done when you have prior knowledge that all variables are significant or you have to or you are prepping for #2.

2. Backward Elimination
	1. Select a significance level (like 0.05)
	2. Fit the full model with all vars
	3. Pick the variable with the highest p-value, if p > sig go to step 4 else finish the model
	4. Eliminate that variable 
	5. Fit the model without that variable

Repeat steps 3-5 until the p-value of the variable chosen in less than sig level. At that stage you have finished the model.

3. Forward selection
	1. Select a sig level
	2. Build simple linear models with each variable. Select one with lowest p-value.
	3. Keep that variable and add an extra predictor.
	4. Consider the predictor (of what is left) with the lowest p-value and go to step 3.
Repeat 3-4 until you reach a predictor that has p-value > sig. Stop and select the model built in the previous step, that is the finished model.

4. Bidirectional elimination (combination of 2 & 3 above)
	1. Select a sig level to enter and a sig level to stay
	2. Perform forward selection.
	3. Perform backward selection checking if any of the added variables can be eliminated.
	4. The model is done when no new variable can enter or leave.

5. Score comparison
	1. Select a goodness of fit criterion
	2. Construct all possible regression models - if there are n vars, there are (2^n -1) models.
	3. Select the one that best fits the criterion.
	